{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths\n",
    "data_train_path = './train_folder'\n",
    "data_test_path = './test_folder'\n",
    "\n",
    "# Classes\n",
    "classes = os.listdir(data_train_path)\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(\"Total number of classes: \" + str(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a357ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "SIZE=IMAGE_SIZE[0]\n",
    "\n",
    "# Carregar o dataset de treino completo (vai ser dividido)\n",
    "full_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=data_train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=classes,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "# Pré-processamento e performance\n",
    "full_train_ds = full_train_ds.shuffle(1024).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Contar batches totais\n",
    "total_batches = tf.data.experimental.cardinality(full_train_ds).numpy()\n",
    "train_batches = int(0.9 * total_batches)\n",
    "val_batches = total_batches - train_batches\n",
    "\n",
    "# Dividir o dataset em treino (90%) e validação (10%)\n",
    "train_ds = full_train_ds.take(train_batches)\n",
    "valid_ds = full_train_ds.skip(train_batches)\n",
    "\n",
    "# Carregar dataset de TESTE (a tua pasta separada)\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=data_test_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=classes,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Verificar tamanhos\n",
    "print(\"Total batches:\", total_batches+tf.data.experimental.cardinality(test_ds).numpy())\n",
    "print(\"Train batches:\", train_batches)\n",
    "print(\"Validation batches:\", val_batches)\n",
    "print(\"Test batches:\", tf.data.experimental.cardinality(test_ds).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e06c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(SIZE, SIZE, 3)),  # 1º bloco\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  # 2º bloco\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),  # 3º bloco\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),  # transforma em vetor\n",
    "    layers.Dense(128, activation='relu'),  # camada densa\n",
    "    layers.Dropout(0.5),  # evita overfitting\n",
    "    layers.Dense(num_classes, activation='softmax')  # saída para classificação\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f56e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model by .fit function\n",
    "history = model.fit(\n",
    "    train_ds,                                          # Dataset to train model\n",
    "    epochs=20,                                        # Number of epochs to train\n",
    "    validation_data=valid_ds                          # Validation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert resutl of training to a DataFrame\n",
    "result_df = pd.DataFrame(history.history)\n",
    "# Show 5 tails of dataframe\n",
    "result_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ebede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a X variable to store range of epochs\n",
    "x = np.arange(len(result_df))\n",
    "\n",
    "# Create a plot with 3 row and 1 col with size of (15, 12)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# AX0 : Loss\n",
    "ax[0].plot(x, result_df.loss, label='loss', linewidth=3)                          \n",
    "ax[0].plot(x, result_df.val_loss, label='val_loss', linewidth=2, ls='-.', c='r')\n",
    "ax[0].set_title('Loss', fontsize=20)\n",
    "ax[0].set_xticks(np.arange(0, len(x), 2))\n",
    "ax[0].legend()\n",
    "\n",
    "#  AX1 : Loss\n",
    "ax[1].plot(x, result_df.accuracy, label='accuracy', linewidth=2)\n",
    "ax[1].plot(x, result_df.val_accuracy, label='val_accuracy', linewidth=2, ls='-.', c='r')\n",
    "ax[1].set_title('Accuracy', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(0, len(x), 2))\n",
    "ax[1].legend()\n",
    "\n",
    "# #  AX2 : Loss\n",
    "# ax[2].plot(x, result_df.learning_rate, label='learning_rate', linewidth=2, marker='o')\n",
    "# ax[2].set_title('learning_rate', fontsize=20)\n",
    "# ax[2].set_xlabel('epochs')\n",
    "# ax[2].set_xticks(np.arange(0, len(x), 2))\n",
    "# ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback, save base model weights in \"MyModel.keras\".\n",
    "# So, we should load it by keras.models.load_model\n",
    "# best_model = tf.keras.models.load_model('MyModel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30db938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model by model.evaluate()\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print()\n",
    "print(f'Loss : {loss}')\n",
    "print(f'Accuracy : {accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c1975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
