{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996a931",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file 'venv\\lib\\site-packages\\typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths\n",
    "data_train_path = './train_folder'\n",
    "data_test_path = './test_folder'\n",
    "\n",
    "# Classes\n",
    "classes = os.listdir(data_train_path)\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(\"Total number of classes: \" + str(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "\n",
    "for n_class in classes:\n",
    "    class_path = os.path.join(data_train_path, n_class)\n",
    "    count = len(os.listdir(class_path))\n",
    "    counts.append((n_class, count))\n",
    "\n",
    "counts = pd.DataFrame(counts, columns=['Class Name', 'Counts'])\n",
    "\n",
    "counts = counts.sort_values(by='Counts', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.barh(counts['Class Name'], counts['Counts'], color='skyblue')\n",
    "plt.xlabel('Counts', fontsize=14)\n",
    "plt.ylabel('Class Name', fontsize=14)\n",
    "plt.title('Counts of Each Class', fontsize=20, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    class_name = classes[i]\n",
    "    class_path = os.path.join(data_train_path, class_name)\n",
    "    \n",
    "    img_files = [f for f in os.listdir(class_path) if f.endswith('.jpg')]\n",
    "    img_path = os.path.join(class_path, img_files[0])\n",
    "    \n",
    "    img = cv.imread(img_path)\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "\n",
    "    ax[row, col].imshow(img)\n",
    "    ax[row, col].axis('off')\n",
    "    ax[row, col].set_title(class_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a357ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "SIZE = IMAGE_SIZE[0]\n",
    "\n",
    "# Load the full training dataset (will be split later)\n",
    "full_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=data_train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=classes,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "# Preprocessing and performance optimization\n",
    "full_train_ds = full_train_ds.shuffle(1024).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Count total number of batches\n",
    "total_batches = tf.data.experimental.cardinality(full_train_ds).numpy()\n",
    "train_batches = int(0.9 * total_batches)\n",
    "val_batches = total_batches - train_batches\n",
    "\n",
    "# Dividir o dataset em treino (90%) e validação (10%)\n",
    "train_ds = full_train_ds.take(train_batches)\n",
    "valid_ds = full_train_ds.skip(train_batches)\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Apply augmentation and normalization to training set\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Apply only normalization to validation set\n",
    "valid_ds = valid_ds.map(lambda x, y: (normalization_layer(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Split the dataset: 90% for training and 10% for validation\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=data_test_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=classes,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Total batches:\", total_batches + tf.data.experimental.cardinality(test_ds).numpy())\n",
    "print(\"Train batches:\", train_batches)\n",
    "print(\"Validation batches:\", val_batches)\n",
    "print(\"Test batches:\", tf.data.experimental.cardinality(test_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e06c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    # Input resizing layer to ensure all input images are of the same size\n",
    "    layers.Resizing(SIZE, SIZE, interpolation=\"bilinear\", input_shape=(SIZE, SIZE, 3)),\n",
    "\n",
    "    # Block 1: First convolutional layer with large receptive field\n",
    "    layers.Conv2D(96, kernel_size=11, strides=4, padding='same'),\n",
    "    layers.Lambda(tf.nn.local_response_normalization),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),\n",
    "\n",
    "    # Block 2: Second convolutional layer with 5x5 kernel\n",
    "    layers.Conv2D(256, kernel_size=5, strides=1, padding='same'),\n",
    "    layers.Lambda(tf.nn.local_response_normalization),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),\n",
    "\n",
    "    # Block 3: Three consecutive convolutional layers with 3x3 kernels\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Conv2D(256, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),\n",
    "\n",
    "    # Fully connected (dense) layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Show the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f56e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model by .fit function\n",
    "history = model.fit(\n",
    "    train_ds,                                          # Dataset to train model\n",
    "    epochs=100,                                        # Number of epochs to train\n",
    "    validation_data=valid_ds                          # Validation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert resutl of training to a DataFrame\n",
    "result_df = pd.DataFrame(history.history)\n",
    "# Show 5 tails of dataframe\n",
    "result_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ebede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a X variable to store range of epochs\n",
    "x = np.arange(len(result_df))\n",
    "\n",
    "# Create a plot with 3 row and 1 col with size of (15, 12)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# AX0 : Loss\n",
    "ax[0].plot(x, result_df.loss, label='loss', linewidth=3)                          \n",
    "ax[0].plot(x, result_df.val_loss, label='val_loss', linewidth=2, ls='-.', c='r')\n",
    "ax[0].set_title('Loss', fontsize=20)\n",
    "ax[0].set_xticks(np.arange(0, len(x), 2))\n",
    "ax[0].legend()\n",
    "\n",
    "#  AX1 : Loss\n",
    "ax[1].plot(x, result_df.accuracy, label='accuracy', linewidth=2)\n",
    "ax[1].plot(x, result_df.val_accuracy, label='val_accuracy', linewidth=2, ls='-.', c='r')\n",
    "ax[1].set_title('Accuracy', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(0, len(x), 2))\n",
    "ax[1].legend()\n",
    "\n",
    "# #  AX2 : Loss\n",
    "# ax[2].plot(x, result_df.learning_rate, label='learning_rate', linewidth=2, marker='o')\n",
    "# ax[2].set_title('learning_rate', fontsize=20)\n",
    "# ax[2].set_xlabel('epochs')\n",
    "# ax[2].set_xticks(np.arange(0, len(x), 2))\n",
    "# ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback, save base model weights in \"MyModel.keras\".\n",
    "# So, we should load it by keras.models.load_model\n",
    "# best_model = tf.keras.models.load_model('MyModel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30db938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate model by model.evaluate()\n",
    "# loss, accuracy = model.evaluate(test_ds)\n",
    "# print()\n",
    "# print(f'Loss : {loss}')\n",
    "# print(f'Accuracy : {accuracy*100}%')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f'Test Loss       : {loss:.4f}')\n",
    "print(f'Test Accuracy   : {accuracy * 100:.2f}%')\n",
    "\n",
    "# Get predictions and true labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "# Compute additional metrics\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print all metrics\n",
    "print()\n",
    "print(\"Additional Evaluation Metrics:\")\n",
    "print(f\"Macro Precision : {precision_macro:.4f}\")\n",
    "print(f\"Macro Recall    : {recall_macro:.4f}\")\n",
    "print(f\"Macro F1 Score  : {f1_macro:.4f}\")\n",
    "print(f\"Micro F1 Score  : {f1_micro:.4f}\")\n",
    "print(f\"Weighted F1     : {f1_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "# Get predictions and true labels from the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = classes  # use the already defined list of class names\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_class = 2\n",
    "total_images = len(classes) * images_per_class\n",
    "rows = (total_images + 2) // 3 \n",
    "\n",
    "fig, ax = plt.subplots(rows, 3, figsize=(12, 4 * rows))\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(data_test_path, class_name)\n",
    "    img_files = [f for f in os.listdir(class_path) if f.endswith('.jpg')]\n",
    "\n",
    "    for j in range(images_per_class):\n",
    "        idx = i * images_per_class + j\n",
    "        if idx >= total_images or j >= len(img_files):\n",
    "            continue  \n",
    "\n",
    "        img_path = os.path.join(class_path, img_files[j])\n",
    "        img = cv.imread(img_path)\n",
    "        img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize and normalize the image for the model\n",
    "        img_resized = cv.resize(img_rgb, IMAGE_SIZE)\n",
    "        input_img = normalization_layer(tf.convert_to_tensor(img_resized[np.newaxis, ...], dtype=tf.float32))[0].numpy()\n",
    "        input_tensor = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "        prediction = model.predict(input_tensor)\n",
    "        pred_class = tf.argmax(prediction, axis=1).numpy()[0]\n",
    "\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "\n",
    "        ax[row, col].imshow(img_rgb)\n",
    "        ax[row, col].axis('off')\n",
    "        ax[row, col].set_title(f\"Real: {class_name}\\nGuess: {classes[pred_class]}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
